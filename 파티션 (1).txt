1)주 파티션
2)확장 파티션
3) 논리 파티션

오토마운트 (Auto Mount) -> 말그대로 자동마운트
- 파일 시스템을 자동으로 마운트해주는기능.

-시스템 종료 과정에서 모든 마운트된 장치는 마운트가 해제가 됩니다.
 그 후 부팅 과정에서 오토 마운트 처리된 장치만 마운트되어 부팅된다.

-마운트 가능한 장치하면  설정을 통해  오토 마운트가 가능하다.
> 기본 설정 파일을 이용한 오토마운트 (/etc/fstab) 
> 서비스를 이용한 오토마운트 ( autofs ) 

* /etc/fstab 구성 -> vi 편집기로 설정.
<file system> : 장치명, 오토 마운트할 파일의 이름을 절대경로로 작성

<mount point> : 오토마운트에 사용할 디렉터리(마운트 포인트)작성

<type> : 오토 마운트에 사용할 장치의 파일 시스템 형식을 작성

<options> : 마운트 옵션, 마운트하며 적용할 옵션을 작성
	    (defaults 옵션으로 기본적인 옵션을 전체 적용 가능하다.)

<dump> : dump 동작 여부. (파일 시스템 백업 도구)

<pass> : fsck 동작 여부 (파일 시스템 체크 도구)

exec/sbin/init (응급모드 리부트)
안될시 물리적 종료

------------------------------------
NFS (Network File System)

- 서로 다른 시스템 간 파일 및 디렉터리를 공유하기 위한 시스템.

-네트워크를 통하여 다른 장치의 파티션을 마운트 할 수 있다.

[패키지 이름]
nfs - kernel- server
[방화벽 포트]
2049/tcp
[설정 파일]
/etc/exports
[설정 내용]
/nfs_server     192.168.108.20(rw,no_root_squash,no_subtree_check,sync)

/nfs_server : nfs로 공유할 디렉터리
	   nfs 는 디렉터리 단위로 시스템을 공유해준다.

192.168.108.20 : nfs로 파일 시스템을 공유 받을 클라이언트.

192.168.108.0/24 
서브넷 마스크 : 255.255.255.0

rw: read&write
ro : read only

root_squash : 클라이언트가 공유받은 시스템에 접근 시 root 권한을 인정치 않는다.
no_root_squash : 클라이언트가 공유받은 시시틈에 접근 시 root 권한을 인정

all_squash: 클라이언트가 공유받은 시스템에 접근 시 일반 사용자 권한 인정 X
no_all_squash: 클라이언트가 공유받은 시스템에 접근 시 일반 사용자 권한 인정 O
(서버, 클라이언트 양측에 대해 동일한 사용자가 필요하다. 사용자명,UID,GID가 일치, 정말 완벽하게 동일한 사용자!)
*권한 인정 X , 익명사용자 Nobody,NoGroup 취급한다

no_subtree_check : 공유된 디렉터리의 하위 디렉터리를 대상으로 명령이 전달되면
		하위 디렉토리에 대한 검사 수행 X

sync : 서버, 클라이언트간 데이터 동기화 

------------------------------
[클라이언트 패키지]
nfs-common

*nfs 마운트 명령어
mount -t nfs [서버IP] : [공유 디렉터리] [마운트 포인트]
ex)
mount -t nfs 192.168.108.10:/nfs_server   /nfs_client

RAID (독립 디스크의 이중화 집합/배열)

-여러 개의 하드 디스크를 하나의 디스크로 결합하여 
  데이터를 저장하고 관리하는 기술

-데이터의 안정성과 성능을 향상시키기 위해 사용

-RAID 구성 방식에 따라서 성능과 용량이 바뀐다.

Linear RAID - 선형 레이드

RAID 0 - Striping

RAID 1 - Mirroring

RAID 2 - 현재 사용 X

RAID 3 - 패리티 비트, 현재 사용 X

RAID 4 - 패리티 비트, 현재 사용 X

RAID 5 - Striping With Parity, 패리티를 사용한 스트라이핑

RAID 6 -  Striping With Dual Parity, 이중 패리티를 사용한 스트라이핑

1) Linear RAID 선형 RAID
-최소 2개이상의 디스크가 필요하다.
-데이터가 순차적으로 여러 디스크에 분산되어 저장된다.
(데이터가 선형으로 저장된다)

-여러 개의 디스크가 하나의 볼륨으로 결합되어 사용된다.
-하나만 고장나도 전부 쓰지 못한다! 안정성은 오히려 떨어져버린다...
ex) Disk a 1,2,3,4,5 / Disk b 6,7,8,9,

2) RAID 0
-최소 2개 이상의 디스크를 필요한다

-데이터를 블록으로 쪼개어 각각 다른 디스크에 나누어 저장한다.

-데이터가 병렬로 저장되어 성능이 뛰어나다.

-한개의 디스크만 고장나도 모든 디스크를 사용할 수 없다.

	DISK 1 	DISK2
	1	2
	3	4
	5	6
	7	8


3) RAID 1 
- 최소 2개의 디스크 사용

- 모든 데이터가 중복되어 저장된다.

- 디스크 전체 용량의 절반밖에 사용하지 못한다.
ex) DISK1 (10G) + DISK2 (10G) = 10 G

- 디스크 하나가 고장나도, 중복되어 저장된 디스크가 하나 더 있어 매우 안전하다

	DISK1 DISK2
	1       1
	2       2
	3       3

4) RAID 4
- 최소 3개의 디스크 사용

- 데이터를 블록으로 쪼개어 각각 다른 디스크에 저장한다

- 디스크에 오류가 발생하면 패리티를 사용하여 복구를 할 수 있다.

- 디스크 하나를 패리티 디스크로 사용한다.

-패리티 디스크의 수명이 짧아져 RAID 5방식이 더 선호된다.
Disk 1 Disk 2  Disk 3 (Parity)
1        2         P(패리티 비트)
3        4         P
5        6         P

5) RAID 5

-데이터를 블록으로 쪼개서 각각 다른 디스크에 저장

-디스켕 오류가 발생하면 패리티 사용하여 복구

-RAID 4 의 단점 개선
매번 다른 디스크에 패리티를 저장한다.
	DISK1 	DISK2	DISK3	
	1	2	P	
	3	P	4	
	P	5	6	
	7	8	P	

* 짝수 패리티, 홀수 패리티
짝수 패리티 : 데이터 비트들의 합이 짝수가 되도록 하는 방법
홀수 패리티 : 데이터 비트들의 합이 홀수가 되도록 하는 방법

	DISK1 	DISK2	DISK3	짝수P	홀수P
	1	0	P	1	0
	1	1	P	0	1
	0	0	P	0	1
	0	1	P	1	0

6) RAID 6 
-최소 4개의 디스크를 사용
-패리티를 두 개 사용한다

-공간 효율은 저하되나 안정성은 더 올라간다
 (디스크가 2개 고장나도 복구 가능)
-패리티를 두개 생성하는 만큼 알고리즘이 복잡하다
  쓰기 성능은 떨어진다
RAID : fdisk 에서 t FD
-----------------------------
mdadm 명령어 
- 리눅스에서 RAID 를 관리
mdadm --create /dev/md10 --level=linear --raid-devices=2 /dev/sdb1 /dev/sdc1


